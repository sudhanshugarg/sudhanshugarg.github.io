## Paper Reading (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift)
[Link](https://arxiv.org/abs/1502.03167)

### Some Intuition behind why batch normalization is useful in practice
 - Reduces training time for a deep learning network
 - Improves gradient flow through the network (helps with vanishing gradient problem)
 - Allows higher learning rates
 - Reduces the strong dependence on initialization
 - Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe

### References
 - [Paper](https://arxiv.org/abs/1502.03167)
 - [cs231n, stanford, Lecture 6, Fei-Fei Li](https://www.youtube.com/watch?v=wEoyxE0GP2M&t=3257s)
 - [Why does batch norm work, Andrew Ng](https://www.youtube.com/watch?v=nUUqwaxLnWs)
 - [Batch Norm at Test Time, Andrew Ng](https://www.youtube.com/watch?v=5qefnAek8OA)